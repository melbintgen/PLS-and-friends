% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Partial least squares (PLS) and friends},
  pdfauthor={JDM and friends},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Partial least squares (PLS) and friends}
\author{JDM and friends}
\date{2024-10-15}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{About}\label{about}

\section{Notation}\label{notation}

\begin{itemize}
\item
  \(a, b\): scalars, i.e., elements of \(\mathbb{R}\), the set of real numbers.
\item
  \(X, Y\): matrices; \(X \in \mathbb{R}^{n\times p}\) is an \(n\) by \(p\) matrix.
\item
  \(\boldsymbol{a}, \boldsymbol{b}\): column vectors (i.e., vectors are by default column vectors); \(\boldsymbol{a} \in \mathbb{R}^{n \times 1}\) has \(n\) elements, i.e., \(\boldsymbol{a} = (a_1,\dots,a_n)^\top\).
\item
  \(\lvert a \rvert\): length of vector \(\boldsymbol{a}\), i.e., \(\lvert \boldsymbol{a} \rvert = \left( \sum_{i=1}^n a_i^2 \right)^{1/2}\).
\item
  \(\angle (\boldsymbol{a}, \boldsymbol{b})\): angle (between \(-90^\circ\) and \(90^\circ\)) between two vectors.
\item
  \(\bar{\boldsymbol{a}}\): average of elements in \(\boldsymbol{a}\), i.e., \(\bar{\boldsymbol{a}} = n^{-1} \sum_{i=1}^n a_i\).
\item
  \(var(\boldsymbol{a})\): variance of elements in \(\boldsymbol{a}\), i.e., \(var(\boldsymbol{a}) = n^{-1} \sum_{i=1}^{n} (a_i - \bar{\boldsymbol{a}})^2\).
\item
  \(X^\top, \boldsymbol{a}^\top\): transpose of matrix and vectors.
\item
  \(\equiv\): always equivalent to, or equivalent by definition; e.g., \(c \equiv \boldsymbol{a}^\top \boldsymbol{b}\) says that \(c\) is defined as the inner product between \(\boldsymbol{a}\) and \(\boldsymbol{b}\).
\end{itemize}

\chapter{Preliminaries}\label{preliminaries}

We briefly review some basics from matrix algebra. We will focus on explaining the geometric intuition behind the concepts and how they are interconnected.

\section{Inner product}\label{inner-product}

The inner product \textbf{aáµ€b} results in a scalar:

\[ 
c \equiv \mathbf{a}^\top \mathbf{b} = \lvert \mathbf{a} \rvert \lvert \mathbf{b} \rvert \cos \{ \angle(\mathbf{a}, \mathbf{b}) \}.
\]

If \(\lvert \mathbf{a} \rvert =  \lvert \mathbf{b} \rvert  = 1\), i.e., \(\mathbf{a}\) and \(\mathbf{b}\) are \emph{unit vectors}, we have

\[
\mathbf{a}^\top \mathbf{b} = \cos \{ \angle(\mathbf{a}, \mathbf{b}) \},
\]

which measures the affinity/similarity between \(\mathbf{a}\) and \(\mathbf{b}\).

To illustrate the idea of affinity/similarity further, if \(\breve{\mathbf{a}}, \breve{\mathbf{b}}\) denote the centered versions of \(\mathbf{a}, \mathbf{b}\), then we actually have (you can work out why)

\[
cov(\breve{\mathbf{a}}, \breve{\mathbf{b}}) \equiv \breve{\mathbf{a}}^\top \breve{\mathbf{b}} = \{var(\breve{\mathbf{a}})var(\breve{\mathbf{b}})\}^{1/2} \cos(\breve{\mathbf{a}}, \breve{\mathbf{b}}).
\]

That is to say \(\cos(\breve{\mathbf{a}}, \breve{\mathbf{b}}) = cor(\breve{\mathbf{a}}, \breve{\mathbf{b}})\).

\section{Inner product as projection}\label{inner-product-as-projection}

Consider the case where \(\mathbf{a}, \mathbf{x} \in \mathbb{R}^n\) with \(\lvert \mathbf{a}\rvert = 1\), then \(\mathbf{x}^\top \mathbf{a}\) defines the length of \(\hat{\mathbf{x}}\), the projection of \(\mathbf{x}\) unto the direction defined by \(\mathbf{a}\).

\section{Matrix product}\label{matrix-product}

Consider two matrices \(A \in \mathbb{R}^{n \times p}\) and \(B \in \mathbb{R}^{p \times r}\). Let \(\mathbf{a}_1^\top, \dots, \mathbf{a}_n^\top \in \mathbb{R}^{1 \times p}\) denote the rows of \(A\) and \(\mathbf{b}_1, \dots, \mathbf{b}_r \in \mathbb{R}^{p \times 1}\) denote the columns of \(B\). Then the product \(AB\) is defined by

\[
AB = 
\left[
\begin{array}{cccc}
    \mathbf{a}_1^\top \mathbf{b}_1 & \mathbf{a}_1^\top \mathbf{b}_2 & \cdots & \mathbf{a}_1^\top \mathbf{b}_r \\
    \vdots & \vdots & & \vdots \\
    \mathbf{a}_n^\top \mathbf{b}_1 & \mathbf{a}_n^\top \mathbf{b}_2 & \cdots & \mathbf{a}_n^\top \mathbf{b}_r 
\end{array}
\right] \equiv \bigl(\mathbf{a}_i^\top \mathbf{b}_j \bigr)_{ij}.
\]

That is, the matrix product can be viewed as a structured way of calculating vector inner products (proximity/similarity).

In the special case where \(X \in \mathbb{R}^{n \times p}\) and \(Y \in \mathbb{R}^{n \times q}\) are column-centered, \(var(X) = X^\top X\) and \(cov(X,Y) = X^\top Y\) are the covariance matrices. That is, proximity of column vectors (variables).

\section{Column (row) space of a matrix}\label{column-row-space-of-a-matrix}

For \(A \in \mathbb{R}^{n \times p}\), the column space of \(A\) is defined as

\[
\operatorname{Col}(A) = \bigl\{ \mathbf \alpha \in \mathbb{R}^{n \times 1} \big | \mathbf \alpha = A\mathbf x \text{ for some } \mathbf x \in \mathbb{R}^{p \times 1} \bigr\}.
\]

Let \(\mathbf{a}_1, \dots, \mathbf{a}_p\) denote the \(p\) columns of \(A\). The above definition states that \(\operatorname{Col}(A)\) contains all vectors \(\mathbf \alpha\) that can be written as \(\mathbf \alpha = x_1 \mathbf{a}_1 + \ldots + x_n \mathbf{a}_n\), i.e., linear combinations of the column vectors of \(A\).

The column vectors \(\mathbf{a}_1, \ldots, \mathbf{a}_n\) therefore define a linear space \(\operatorname{Col}(A)\), i.e., a collection of vectors whose arbitrary linear combinations are also in that collection. We refer to \(\mathbf{a}_1, \ldots, \mathbf{a}_n\) as the \emph{basis} of \(\operatorname{Col}(A)\); we also say that the basis vectors \(\mathbf{a}_1, \ldots, \mathbf{a}_n\) \emph{span} the space \(\operatorname{Col}(A)\).

\section{Rank of a matrix}\label{rank-of-a-matrix}

Sometimes a basis can contain redundant basis vectors. For example, consider a matrix \(A = (\mathbf{a}_1, \mathbf{a}_2, \mathbf{a}_3)\) where \(\mathbf{a}_2 = 0.4\mathbf{a}_1 + 0.6\mathbf{a}_3\). Then, in terms of defining the column space \(\operatorname{Col}(A)\), \(\mathbf{a}_2\) is redundant. In this case, we say that \(A\) is not of \emph{full rank}, or that \(A\) is \emph{rank deficient}. The rank of \(A\), denoted by \(rank(A)\), is defined as the number of \emph{linearly independent} column vectors. (It is also equivalent to the number of linearly independent \emph{row} vectors.) The value of \(rank(A)\) gives us a good indicator of the ``true dimensionality'' of \(A\).

In biology, suppose we have a gene expression matrix \(X \in \mathbb{R}^{n \times p}\) with normalized values. Then \(X\) not having full rank means that the expression level of some gene can be perfectly predicted as the linear combinations of other genes. We can safely exclude this redundant gene from analysis without causing any information loss. In reality, however, due to observational noise, we can rarely have a strictly rank deficient gene expression matrix. Instead, it is often useful to assume that the observed gene expression \(X\) is an additive combination of signal and noise, i.e., \(X = A + E\), where \(A\) is a low-rank signal matrix and \(E\) is a full rank noise matrix.

\section{Matrix as linear transform}\label{matrix-as-linear-transform}

With the definition of column and row spaces in mind, we can view a matrix \(A \in \mathbb{R}^{n \times p}\) as defining a linear transform from \(\operatorname{A}\) to \(\operatorname{Row(A)}\), i.e., from \(\mathbb{R}^{p}\) to \(\mathbb{R}^n\). Taking \(\mathbf x \in \mathbb{R}^{p \times 1}\), \(A\mathbf x\) defines a vector that lives in \(\mathbb{R}^n\).

For \(X \in \mathbb{R}^{n \times p}\) and \(Y \in \mathbb{R}^{n \times K}\).

\section{Orthogonalisation}\label{orthogonalisation}

In data analysis

\section{Orthogonal bases}\label{orthogonal-bases}

Suppose \(A \in \mathbb{R}^{n \times p}\) and \(r \equiv rank(A) < p < n\). Then we can find a subset of \(r\) linearly independent column vectors \(\mathbf{a}_{i(1)}, \dots, \mathbf{a}_{i(r)}\), with \(i(1), \dots, i(r) \in \{1, \dots, p\}\), forming a basis for \(\operatorname{Col}(A)\). In general, any set of \(r\) linearly independent vectors living in \(\operatorname{Col}(A)\) can serve as a (non-redundant) basis of \(\operatorname{Col}(A)\). It is often more convenient to find an \emph{orthogonal} basis: when we plot a 3D space, we usually try to render 3 vectors that are perpendicular to each other, e.g., \((1,0,0)\), \((0,1,0)\), and \((0,0,1)\).

\section{Singular value decomposition}\label{singular-value-decomposition}

\end{document}
